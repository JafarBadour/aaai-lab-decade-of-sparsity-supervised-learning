{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AAAI 2026 Tutorial: A Decade of Sparse Training\n",
        "\n",
        "## Dynamic Sparse Training (DST) Tutorial\n",
        "\n",
        "This notebook demonstrates Dynamic Sparse Training concepts and implementations.\n",
        "\n",
        "**Learning Objectives:**\n",
        "1. Understand the difference between simulated and truly sparse implementations\n",
        "2. Implement DST algorithms from scratch\n",
        "3. Experience the performance-efficiency trade-offs\n",
        "4. Recognize system-level barriers to sparse training adoption\n",
        "\n",
        "**Dataset:** MNIST (handwritten digits)\n",
        "- 60,000 training samples, 10,000 test samples\n",
        "- 28x28 grayscale images, 10 classes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "\n",
        "# Import utilities from utils.py\n",
        "from utils import (\n",
        "    SimpleNet, MaskedNet,\n",
        "    train_epoch, evaluate,\n",
        "    apply_dst_step,\n",
        "    benchmark_inference,\n",
        "    plot_mask_evolution_per_epoch,\n",
        "    plot_mask_2d_evolution,\n",
        "    plot_dense_vs_masked_comparison,\n",
        "    CUPY_GPU_AVAILABLE\n",
        ")\n",
        "\n",
        "print(\"✓ All imports successful!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "print(f\"CuPy GPU available: {CUPY_GPU_AVAILABLE}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "Set hyperparameters and global settings here.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Inform about CuPy GPU availability\n",
        "if CUPY_GPU_AVAILABLE:\n",
        "    print(f\"✓ CuPy GPU acceleration available - will use GPU CSR matrices\")\n",
        "else:\n",
        "    print(f\"⚠ CuPy GPU not available - will use CPU CSR matrices\")\n",
        "\n",
        "# Hyperparameters\n",
        "GLOBAL_SPARSITY = 0.95  # 95% sparsity (adjust to experiment)\n",
        "batch_size = 64\n",
        "num_epochs = 10\n",
        "learning_rate = 0.001\n",
        "dst_frequency = 2  # Apply DST every N epochs\n",
        "\n",
        "print(f\"\\nHyperparameters:\")\n",
        "print(f\"  Sparsity: {GLOBAL_SPARSITY*100}%\")\n",
        "print(f\"  Batch size: {batch_size}\")\n",
        "print(f\"  Epochs: {num_epochs}\")\n",
        "print(f\"  Learning rate: {learning_rate}\")\n",
        "print(f\"  DST frequency: every {dst_frequency} epochs\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load MNIST Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))  # MNIST mean and std\n",
        "])\n",
        "\n",
        "# Download and load training data\n",
        "train_dataset = torchvision.datasets.MNIST(\n",
        "    root='./data', \n",
        "    train=True, \n",
        "    download=True, \n",
        "    transform=transform\n",
        ")\n",
        "train_loader = DataLoader(\n",
        "    train_dataset, \n",
        "    batch_size=batch_size, \n",
        "    shuffle=True,\n",
        "    num_workers=2 if torch.cuda.is_available() else 0\n",
        ")\n",
        "\n",
        "# Download and load test data\n",
        "test_dataset = torchvision.datasets.MNIST(\n",
        "    root='./data', \n",
        "    train=False, \n",
        "    download=True, \n",
        "    transform=transform\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_dataset, \n",
        "    batch_size=batch_size, \n",
        "    shuffle=False,\n",
        "    num_workers=2 if torch.cuda.is_available() else 0\n",
        ")\n",
        "\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Test samples: {len(test_dataset)}\")\n",
        "print(f\"Image size: {train_dataset[0][0].shape}\")\n",
        "print(f\"Number of classes: {len(train_dataset.classes)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Create Networks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create base network\n",
        "base_net = SimpleNet(num_classes=10)\n",
        "print(f\"Base network created with {sum(p.numel() for p in base_net.parameters())} parameters\")\n",
        "\n",
        "# Create dense network (copy of base)\n",
        "dense_net = copy.deepcopy(base_net).to(device)\n",
        "\n",
        "# Create masked network\n",
        "print(f\"\\nCreating masked network with {GLOBAL_SPARSITY*100}% sparsity\")\n",
        "masked_net = MaskedNet(base_net, sparsity=GLOBAL_SPARSITY).to(device)\n",
        "\n",
        "# Print sparsity statistics\n",
        "stats = masked_net.get_sparsity_stats()\n",
        "print(\"\\nSparsity Statistics:\")\n",
        "for name, stat in stats.items():\n",
        "    print(f\"  {name}: {stat['sparsity']*100:.2f}% sparse ({stat['zeros']}/{stat['total']} zeros)\")\n",
        "\n",
        "# Loss and optimizers\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "dense_optimizer = optim.Adam(dense_net.parameters(), lr=learning_rate)\n",
        "masked_optimizer = optim.Adam(masked_net.parameters(), lr=learning_rate)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Training Loop\n",
        "\n",
        "Train both dense and masked networks, applying Dynamic Sparse Training periodically.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Track metrics for both networks\n",
        "dense_train_losses = []\n",
        "dense_train_accs = []\n",
        "dense_test_losses = []\n",
        "dense_test_accs = []\n",
        "\n",
        "masked_train_losses = []\n",
        "masked_train_accs = []\n",
        "masked_test_losses = []\n",
        "masked_test_accs = []\n",
        "sparsity_per_epoch = []  # Track sparsity per layer per epoch\n",
        "masks_per_epoch = []  # Track actual mask values per epoch for visualization\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"Training Dense and Dense+Mask Networks\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Train dense network\n",
        "    dense_train_loss, dense_train_acc = train_epoch(dense_net, train_loader, criterion, dense_optimizer, device)\n",
        "    dense_train_losses.append(dense_train_loss)\n",
        "    dense_train_accs.append(dense_train_acc)\n",
        "    dense_test_acc, dense_test_loss = evaluate(dense_net, test_loader, device, criterion)\n",
        "    dense_test_accs.append(dense_test_acc)\n",
        "    dense_test_losses.append(dense_test_loss)\n",
        "    \n",
        "    # Train masked network\n",
        "    masked_train_loss, masked_train_acc = train_epoch(masked_net, train_loader, criterion, masked_optimizer, device)\n",
        "    masked_train_losses.append(masked_train_loss)\n",
        "    masked_train_accs.append(masked_train_acc)\n",
        "    masked_test_acc, masked_test_loss = evaluate(masked_net, test_loader, device, criterion)\n",
        "    masked_test_accs.append(masked_test_acc)\n",
        "    masked_test_losses.append(masked_test_loss)\n",
        "    \n",
        "    # Track sparsity per epoch (per layer)\n",
        "    stats = masked_net.get_sparsity_stats()\n",
        "    sparsity_dict = {name: stat['sparsity'] for name, stat in stats.items()}\n",
        "    sparsity_per_epoch.append(sparsity_dict)\n",
        "    \n",
        "    # Track mask values per epoch for visualization\n",
        "    masks_2d = masked_net.get_masks_as_2d()\n",
        "    masks_per_epoch.append(masks_2d)\n",
        "    \n",
        "    # Apply DST periodically\n",
        "    if (epoch + 1) % dst_frequency == 0:\n",
        "        print(f\"\\nEpoch {epoch+1}: Applying Dynamic Sparse Training...\")\n",
        "        apply_dst_step(masked_net, prune_ratio=0.1)\n",
        "        \n",
        "        # Update sparsity statistics\n",
        "        stats = masked_net.get_sparsity_stats()\n",
        "        avg_sparsity = np.mean([s['sparsity'] for s in stats.values()])\n",
        "        \n",
        "        print(f\"  Average sparsity after DST: {avg_sparsity*100:.2f}%\")\n",
        "    \n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
        "    print(f\"  Dense:      Train Loss: {dense_train_loss:.4f}, Train Acc: {dense_train_acc:.2f}%, Test Loss: {dense_test_loss:.4f}, Test Acc: {dense_test_acc:.2f}%\")\n",
        "    print(f\"  Dense+Mask: Train Loss: {masked_train_loss:.4f}, Train Acc: {masked_train_acc:.2f}%, Test Loss: {masked_test_loss:.4f}, Test Acc: {masked_test_acc:.2f}%\")\n",
        "    print(f\"  Difference: Loss Δ: {masked_test_loss - dense_test_loss:+.4f}, Acc Δ: {masked_test_acc - dense_test_acc:+.2f}%\")\n",
        "\n",
        "print(\"\\n✓ Training complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Final Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "final_test_acc_dense, final_test_loss_dense = evaluate(dense_net, test_loader, device, criterion)\n",
        "final_test_acc_masked, final_test_loss_masked = evaluate(masked_net, test_loader, device, criterion)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"Final Test Results\")\n",
        "print(\"=\"*60)\n",
        "print(f\"  Dense:      Loss: {final_test_loss_dense:.4f}, Acc: {final_test_acc_dense:.2f}%\")\n",
        "print(f\"  Dense+Mask: Loss: {final_test_loss_masked:.4f}, Acc: {final_test_acc_masked:.2f}%\")\n",
        "print(f\"  Difference: Loss Δ: {final_test_loss_masked - final_test_loss_dense:+.4f}, Acc Δ: {final_test_acc_masked - final_test_acc_dense:+.2f}%\")\n",
        "\n",
        "final_stats = masked_net.get_sparsity_stats()\n",
        "print(\"\\nFinal Sparsity Statistics:\")\n",
        "for name, stat in final_stats.items():\n",
        "    print(f\"  {name}: {stat['sparsity']*100:.2f}% sparse ({stat['zeros']}/{stat['total']} zeros)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Generate Visualizations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"Generating plots...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Plot mask evolution per epoch (sparsity line plot)\n",
        "plot_mask_evolution_per_epoch(sparsity_per_epoch, masks_per_epoch, save_path='plots/mask_evolution.png')\n",
        "\n",
        "# Plot 2D mask evolution as images\n",
        "plot_mask_2d_evolution(masks_per_epoch, save_path='plots/mask_2d_evolution.png')\n",
        "\n",
        "# Plot dense vs masked comparison\n",
        "plot_dense_vs_masked_comparison(\n",
        "    dense_train_losses, dense_train_accs, dense_test_losses, dense_test_accs,\n",
        "    masked_train_losses, masked_train_accs, masked_test_losses, masked_test_accs,\n",
        "    save_path='plots/dense_vs_masked.png'\n",
        ")\n",
        "\n",
        "print(\"\\n✓ All plots saved to 'plots/' directory\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Benchmarking - Simulated vs Truly Sparse\n",
        "\n",
        "Compare inference time for different implementations to demonstrate the difference between simulated and truly sparse approaches.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"Benchmarking - Simulated vs Truly Sparse\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nComparing inference time for different implementations...\")\n",
        "print(\"(This demonstrates the difference between simulated and truly sparse)\")\n",
        "\n",
        "benchmark_results = benchmark_inference(masked_net, test_loader, device, num_samples=100)\n",
        "\n",
        "print(\"\\nInference Time Comparison (100 samples, 10 runs):\")\n",
        "print(\"-\" * 60)\n",
        "for method, timing in benchmark_results.items():\n",
        "    print(f\"{method:30s}: {timing['mean']:6.2f} ± {timing['std']:5.2f} ms\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Speedup Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"Speedup Analysis\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# GPU comparison: GPU Dense+Mask vs GPU Truly Sparse (if available)\n",
        "if 'Dense+Mask (GPU)' in benchmark_results and 'Truly Sparse CSR (GPU)' in benchmark_results:\n",
        "    gpu_dense = benchmark_results['Dense+Mask (GPU)']['mean']\n",
        "    gpu_sparse = benchmark_results['Truly Sparse CSR (GPU)']['mean']\n",
        "    speedup = gpu_dense / gpu_sparse if gpu_sparse > 0 else 0\n",
        "    print(f\"\\nMain Comparison: GPU Dense+Mask vs GPU Truly Sparse\")\n",
        "    print(f\"  Dense+Mask (GPU): {gpu_dense:.2f} ms\")\n",
        "    print(f\"  Truly Sparse CSR (GPU): {gpu_sparse:.2f} ms\")\n",
        "    if speedup > 1:\n",
        "        print(f\"  → GPU Truly Sparse is {speedup:.2f}x faster than GPU Dense+Mask!\")\n",
        "    elif speedup < 1:\n",
        "        print(f\"  → GPU Dense+Mask is {1/speedup:.2f}x faster\")\n",
        "    else:\n",
        "        print(f\"  → Similar performance\")\n",
        "    print(f\"\\n  Note: This demonstrates the efficiency gains of truly sparse\")\n",
        "    print(f\"        implementations on GPU using cuSPARSE acceleration.\")\n",
        "\n",
        "# Cross-platform comparison: GPU Dense+Mask vs CPU Truly Sparse\n",
        "if 'Dense+Mask (GPU)' in benchmark_results and 'Truly Sparse CSR (CPU)' in benchmark_results:\n",
        "    gpu_dense = benchmark_results['Dense+Mask (GPU)']['mean']\n",
        "    cpu_sparse = benchmark_results['Truly Sparse CSR (CPU)']['mean']\n",
        "    speedup = gpu_dense / cpu_sparse if cpu_sparse > 0 else 0\n",
        "    print(f\"\\nCross-Platform Comparison: GPU Dense+Mask vs CPU Truly Sparse\")\n",
        "    print(f\"  Dense+Mask (GPU): {gpu_dense:.2f} ms\")\n",
        "    print(f\"  Truly Sparse CSR (CPU): {cpu_sparse:.2f} ms\")\n",
        "    if speedup > 1:\n",
        "        print(f\"  → CPU Truly Sparse is {speedup:.2f}x faster than GPU Dense+Mask!\")\n",
        "    elif speedup < 1:\n",
        "        print(f\"  → GPU Dense+Mask is {1/speedup:.2f}x faster\")\n",
        "    else:\n",
        "        print(f\"  → Similar performance\")\n",
        "    print(f\"\\n  Note: This shows that truly sparse implementations on CPU\")\n",
        "    print(f\"        can be competitive with dense implementations on GPU\")\n",
        "    print(f\"        for high sparsity levels.\")\n",
        "\n",
        "# CPU comparison (for reference)\n",
        "if 'Dense+Mask (CPU)' in benchmark_results and 'Truly Sparse CSR (CPU)' in benchmark_results:\n",
        "    cpu_dense = benchmark_results['Dense+Mask (CPU)']['mean']\n",
        "    cpu_sparse = benchmark_results['Truly Sparse CSR (CPU)']['mean']\n",
        "    speedup = cpu_dense / cpu_sparse if cpu_sparse > 0 else 0\n",
        "    print(f\"\\nCPU Comparison (for reference):\")\n",
        "    print(f\"  Dense+Mask (CPU): {cpu_dense:.2f} ms\")\n",
        "    print(f\"  Truly Sparse CSR (CPU): {cpu_sparse:.2f} ms\")\n",
        "    print(f\"  Speedup: {speedup:.2f}x\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "### Key Concepts Demonstrated:\n",
        "1. ✓ Simulated sparsity using binary masks (Dense+Mask)\n",
        "2. ✓ Masked Conv2d and Linear layers\n",
        "3. ✓ Dynamic Sparse Training (pruning + regrowing)\n",
        "4. ✓ Gradient masking to prevent updates to pruned weights\n",
        "5. ✓ Truly sparse implementation using CSR matrices\n",
        "6. ✓ Benchmarking comparison: Dense+Mask vs Truly Sparse\n",
        "\n",
        "### Understanding the Limitations\n",
        "\n",
        "**THE PARADOX: Why Do We Still Stick to Dense Training?**\n",
        "\n",
        "Despite DST being algorithmically mature and often outperforming dense training, most implementations remain dense. Here's why:\n",
        "\n",
        "1. **SIMULATED SPARSITY (What We Just Implemented):**\n",
        "   - Uses binary masks over dense weights\n",
        "   - Convenient: works with existing frameworks\n",
        "   - Problem: Still stores ALL weights in memory\n",
        "   - Problem: Still computes with dense kernels\n",
        "   - Result: NO memory savings, minimal speedup\n",
        "\n",
        "2. **TRULY SPARSE IMPLEMENTATIONS:**\n",
        "   - Uses sparse matrix formats (COO, CSR, CSC)\n",
        "   - Stores only non-zero weights\n",
        "   - Requires custom kernels for efficiency\n",
        "   - Challenge: Hardware optimized for dense operations\n",
        "   - Challenge: Requires significant engineering effort\n",
        "\n",
        "3. **SYSTEM-LEVEL BARRIERS:**\n",
        "   - Hardware: GPUs optimized for dense matrix multiplication\n",
        "   - Software: Deep learning frameworks favor dense operations\n",
        "   - Memory: Sparse formats have overhead for small sparsity\n",
        "   - Engineering: Truly sparse implementations are complex\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Understand sparse matrix formats and operations (COO, CSR, CSC)\n",
        "- Consider hardware-aware sparsity patterns\n",
        "- Join the sparse training community for collaboration\n",
        "\n",
        "For more information, see the AAAI 2026 Tutorial:\n",
        "**\"A Decade of Sparse Training: Why Do We Still Stick to Dense Training?\"**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
